# âœ¨ğŸ“š Google AI Destekli Ã‡oklu Sohbet PDF AsistanÄ± / Google AI Powered Multi-Session PDF Assistant ğŸ“„

Bu Streamlit uygulamasÄ±, kullanÄ±cÄ±larÄ±n PDF dosyalarÄ± yÃ¼kleyerek bu dosyalarÄ±n iÃ§eriÄŸi hakkÄ±nda sorular sormasÄ±na olanak tanÄ±r. Genel sorular sorulduÄŸunda ("Bu PDF ne hakkÄ±nda?"), uygulama PDF iÃ§eriÄŸinden bir Ã¶zet veya Ã¶rnek sorular tÃ¼retmeye Ã§alÄ±ÅŸÄ±r. Spesifik sorular iÃ§in ise, yÃ¼klenen PDF'leri kaynak olarak kullanarak yanÄ±tlar Ã¼retir. Uygulama, her bir PDF seti iÃ§in ayrÄ± sohbet oturumlarÄ± oluÅŸturur ve yÃ¶netir.

This Streamlit application allows users to upload PDF files and ask questions about their content. When general questions are asked (e.g., "What is this PDF about?"), the application attempts to generate a summary or sample questions from the PDF content. For specific questions, it generates answers using the uploaded PDFs as a source. The application creates and manages separate chat sessions for each set of PDFs.

---

## ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e AÃ§Ä±klama

### âœ¨ Ã–ne Ã‡Ä±kan Ã–zellikler

*   **PDF YÃ¼kleme:** Bir veya daha fazla PDF dosyasÄ± yÃ¼klenebilir.
*   **Ä°Ã§erik TabanlÄ± Soru Cevaplama (RAG):** YÃ¼klenen PDF'lerin iÃ§eriÄŸine dayalÄ± olarak spesifik sorulara yanÄ±t verir.
*   **Genel Soru Anlama ve YanÄ±tlama:** "Bu PDF ne hakkÄ±nda?" gibi genel sorulara, PDF'ten Ã¶zet veya Ã¶rnek sorular tÃ¼reterek yanÄ±t vermeye Ã§alÄ±ÅŸÄ±r.
    *   Bu Ã¶zellik iÃ§in **Langchain PydanticOutputParser** kullanÄ±larak yapÄ±landÄ±rÄ±lmÄ±ÅŸ Ã§Ä±ktÄ± (JSON) hedeflenir.
*   **Ã‡oklu Sohbet OturumlarÄ±:** Her PDF seti veya baÅŸlatÄ±lan sohbet iÃ§in ayrÄ± oturumlar oluÅŸturulabilir, seÃ§ilebilir ve yÃ¶netilebilir.
*   **Google Generative AI Entegrasyonu:**
    *   **LLM:** Metin Ã¼retimi ve sorularÄ± yanÄ±tlama iÃ§in Google'Ä±n Gemini modelleri (Ã¶rn: `gemini-1.5-flash-latest`) kullanÄ±lÄ±r.
    *   **Embedding:** Metinleri vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in Google'Ä±n embedding modelleri (Ã¶rn: `models/embedding-001`) kullanÄ±lÄ±r.
*   **Sohbet GeÃ§miÅŸi:** Her oturum iÃ§in ayrÄ± sohbet geÃ§miÅŸi tutulur.
*   **KÄ±sÄ±tlayÄ±cÄ± Prompt MÃ¼hendisliÄŸi:** LLM'in sadece yÃ¼klenen PDF iÃ§eriÄŸine odaklanmasÄ±nÄ± saÄŸlamak ve dÄ±ÅŸarÄ±dan bilgi kullanmasÄ±nÄ± engellemek iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ prompt ÅŸablonlarÄ± kullanÄ±lÄ±r.

### ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

1.  **Depoyu KlonlayÄ±n:**
    ```bash
    git clone https://github.com/SENIN-KULLANICI-ADIN/SENIN-REPO-ADIN.git
    cd SENIN-REPO-ADIN
    ```

2.  **Sanal Ortam OluÅŸturun (Ã–nerilir):**
    ```bash
    python -m venv venv
    # Windows iÃ§in:
    venv\Scripts\activate
    # macOS/Linux iÃ§in:
    source venv/bin/activate
    ```

3.  **Gerekli KÃ¼tÃ¼phaneleri YÃ¼kleyin:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Google API AnahtarÄ±nÄ± AyarlayÄ±n (Streamlit Secrets):**
    Projenizin ana dizininde `.streamlit` adÄ±nda bir klasÃ¶r oluÅŸturun ve iÃ§ine `secrets.toml` adÄ±nda bir dosya ekleyin.
    ```toml
    # .streamlit/secrets.toml

    GOOGLE_API_KEY = "SENIN_GOOGLE_AI_STUDIO_API_ANAHTARIN"

    # Ä°steÄŸe baÄŸlÄ±: KullanÄ±lacak Google AI modellerini deÄŸiÅŸtirmek iÃ§in
    # GOOGLE_LLM_MODEL_NAME = "gemini-1.5-flash-latest"
    # GOOGLE_EMBEDDING_MODEL_NAME = "models/embedding-001"
    ```
    **Not:** `SENIN_GOOGLE_AI_STUDIO_API_ANAHTARIN` kÄ±smÄ±nÄ± kendi Google AI Studio'dan aldÄ±ÄŸÄ±nÄ±z API anahtarÄ±nÄ±zla deÄŸiÅŸtirin.

5.  **UygulamayÄ± Ã‡alÄ±ÅŸtÄ±rÄ±n:**
    ```bash
    streamlit run app.py
    ```
    Uygulama varsayÄ±lan olarak `http://localhost:8501` adresinde aÃ§Ä±lacaktÄ±r.

### ğŸ› ï¸ NasÄ±l Ã‡alÄ±ÅŸÄ±r?

1.  KullanÄ±cÄ± bir sohbet oturumu baÅŸlatÄ±r veya mevcut birini seÃ§er.
2.  SeÃ§ili oturum iÃ§in bir veya daha fazla PDF dosyasÄ± yÃ¼kler ve "Ä°ÅŸle" butonuna tÄ±klar.
3.  Uygulama, PDF'lerden metinleri Ã§Ä±karÄ±r ve daha kÃ¼Ã§Ã¼k parÃ§alara (chunks) bÃ¶ler. Bu chunk'lar hem RAG iÃ§in hem de genel sorulara yanÄ±t iÃ§in saklanÄ±r.
4.  Google'Ä±n embedding modeli kullanÄ±larak bu metin parÃ§alarÄ± vektÃ¶rlere (embeddings) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.
5.  Bu vektÃ¶rler, hÄ±zlÄ± benzerlik aramasÄ± iÃ§in bir FAISS vektÃ¶r deposunda saklanÄ±r (aktif oturuma Ã¶zel).
6.  KullanÄ±cÄ± bir soru sorduÄŸunda:
    *   **Genel Soru Ä°se:** KullanÄ±cÄ±nÄ±n sorusu genel bir ifade iÃ§eriyorsa (Ã¶rn: "PDF ne hakkÄ±nda?"), saklanan metin chunk'larÄ±nÄ±n bir kÄ±smÄ± ve yapÄ±landÄ±rÄ±lmÄ±ÅŸ Ã§Ä±ktÄ± (Ã¶zet veya Ã¶rnek sorular) iÃ§in Ã¶zel bir prompt ÅŸablonu kullanÄ±larak Google Gemini LLM'ine istek gÃ¶nderilir. YanÄ±t, PydanticOutputParser ile parse edilerek kullanÄ±cÄ±ya sunulur.
    *   **Spesifik Soru Ä°se (RAG):** Sorunun vektÃ¶rÃ¼ne en yakÄ±n olan metin parÃ§alarÄ± (ilgili baÄŸlam) FAISS vektÃ¶r deposundan alÄ±nÄ±r. Bu baÄŸlam ve kullanÄ±cÄ±nÄ±n sorusu, LLM'i sadece saÄŸlanan bilgiyi kullanmaya yÃ¶nlendiren kÄ±sÄ±tlayÄ±cÄ± bir RAG prompt ÅŸablonu kullanÄ±larak formatlanÄ±r. FormatlanmÄ±ÅŸ prompt, Google Gemini LLM'ine gÃ¶nderilir ve yanÄ±t kullanÄ±cÄ±ya gÃ¶sterilir.
7.  Her sohbet oturumunun kendi PDF bilgisi, vektÃ¶r deposu ve sohbet geÃ§miÅŸi ayrÄ± olarak tutulur.

---

## ğŸ‡¬ğŸ‡§ğŸ‡ºğŸ‡¸ English Description

### âœ¨ Key Features

*   **PDF Upload:** Allows uploading one or more PDF files.
*   **Content-Based Q&A (RAG):** Answers specific questions based on the content of the uploaded PDFs.
*   **General Query Understanding and Response:** When asked general questions (e.g., "What is this PDF about?"), it attempts to generate a summary or sample questions from the PDF content.
    *   Utilizes **Langchain PydanticOutputParser** for structured output (JSON) for this feature.
*   **Multi-Session Chat:** Create, select, and manage separate chat sessions for each set of PDFs or initiated chats.
*   **Google Generative AI Integration:**
    *   **LLM:** Uses Google's Gemini models (e.g., `gemini-1.5-flash-latest`) for text generation and answering questions.
    *   **Embedding:** Uses Google's embedding models (e.g., `models/embedding-001`) to convert text into vectors.
*   **Chat History:** Maintains a separate chat history for each session.
*   **Constrained Prompt Engineering:** Utilizes specifically designed prompt templates to ensure the LLM focuses solely on the uploaded PDF content and prevents it from using external knowledge.

### ğŸš€ Setup and Running

1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/YOUR-USERNAME/YOUR-REPO-NAME.git
    cd YOUR-REPO-NAME
    ```

2.  **Create a Virtual Environment (Recommended):**
    ```bash
    python -m venv venv
    # For Windows:
    venv\Scripts\activate
    # For macOS/Linux:
    source venv/bin/activate
    ```

3.  **Install Required Libraries:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set Up Google API Key (Streamlit Secrets):**
    In your project's root directory, create a folder named `.streamlit` and add a file named `secrets.toml` inside it.
    ```toml
    # .streamlit/secrets.toml

    GOOGLE_API_KEY = "YOUR_GOOGLE_AI_STUDIO_API_KEY"

    # Optional: To change the Google AI models used
    # GOOGLE_LLM_MODEL_NAME = "gemini-1.5-flash-latest"
    # GOOGLE_EMBEDDING_MODEL_NAME = "models/embedding-001"
    ```
    **Note:** Replace `YOUR_GOOGLE_AI_STUDIO_API_KEY` with your actual API key obtained from Google AI Studio.

5.  **Run the Application:**
    ```bash
    streamlit run app.py
    ```
    The application will typically open at `http://localhost:8501`.

### ğŸ› ï¸ How It Works

1.  The user starts a new chat session or selects an existing one.
2.  For the selected session, the user uploads one or more PDF files and clicks "Process."
3.  The application extracts text from the PDFs and splits it into smaller chunks. These chunks are stored for both RAG and answering general questions.
4.  These text chunks are converted into vector embeddings using Google's embedding model.
5.  These vectors are stored in a FAISS vector store, specific to the active session, for efficient similarity searches.
6.  When the user asks a question:
    *   **If it's a General Question:** If the user's query contains general phrasing (e.g., "What is this PDF about?"), a portion of the stored text chunks and a special prompt template for structured output (summary or sample questions) are used to send a request to the Google Gemini LLM. The response is parsed using PydanticOutputParser and presented to the user.
    *   **If it's a Specific Question (RAG):** The text chunks most similar to the question's vector (relevant context) are retrieved from the FAISS vector store. This context and the user's question are formatted using a constrained RAG prompt template that guides the LLM to use only the provided information. The formatted prompt is sent to the Google Gemini LLM, and the response is displayed.
7.  Each chat session maintains its own PDF information, vector store, and chat history separately.

---

### âš™ï¸ `requirements.txt` Ä°Ã§eriÄŸi / Content

```txt
streamlit
pypdf2
langchain
langchain-google-genai
faiss-cpu
tiktoken
numpy<2.0
pydantic
uuid
langchain_community
