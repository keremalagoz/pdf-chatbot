import streamlit as st
from openai import OpenAI
import os
from PyPDF2 import PdfReader # PDF okumak iÃ§in
from langchain.text_splitter import RecursiveCharacterTextSplitter # Metin bÃ¶lmek iÃ§in
from langchain_openai import OpenAIEmbeddings # Embedding oluÅŸturmak iÃ§in
from langchain_community.vectorstores import FAISS # VektÃ¶r veritabanÄ± iÃ§in
from langchain.chains.question_answering import load_qa_chain # Soru cevap zinciri
from langchain_core.prompts import PromptTemplate # Prompt ÅŸablonu iÃ§in

# --- Streamlit Secrets ve OpenRouter KonfigÃ¼rasyonu ---
OPENROUTER_API_KEY = st.secrets.get("OPENROUTER_API_KEY")
# LLM modeli (sohbet iÃ§in)
LLM_MODEL_NAME = st.secrets.get("LLM_MODEL_NAME", "mistralai/mistral-7b-instruct:free")
# Embedding modeli (metinleri vektÃ¶re Ã§evirmek iÃ§in, OpenRouter'da OpenAI uyumlu bir model olmalÄ±)
# Genellikle text-embedding-ada-002 veya benzeri bir model OpenRouter'da bulunur.
# OpenRouter model listesinden uygun bir embedding modeli seÃ§in.
EMBEDDING_MODEL_NAME = st.secrets.get("EMBEDDING_MODEL_NAME", "openai/text-embedding-ada-002") # VEYA "text-embedding-ada-002"

if not OPENROUTER_API_KEY:
    st.error("OpenRouter API anahtarÄ± bulunamadÄ±! LÃ¼tfen Streamlit Secrets bÃ¶lÃ¼mÃ¼ne 'OPENROUTER_API_KEY' olarak ekleyin.")
    st.stop()

# OpenRouter iÃ§in OpenAI istemcisi (HEM LLM HEM DE EMBEDDING Ä°Ã‡Ä°N KULLANILACAK)
# Langchain OpenAIEmbeddings, bu client'Ä± doÄŸrudan kullanamayabilir,
# bu yÃ¼zden embedding iÃ§in ayrÄ±ca parametreleri geÃ§eceÄŸiz.
llm_client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)

# Langchain iÃ§in OpenAI Embeddings yapÄ±landÄ±rmasÄ± (OpenRouter Ã¼zerinden)
# Not: Langchain'in OpenAIEmbeddings sÄ±nÄ±fÄ± bazen 'deployment' veya 'model' bekler.
# OpenRouter iÃ§in model adÄ±nÄ± doÄŸrudan 'model' parametresi ile veriyoruz.
try:
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL_NAME,
        openai_api_base="https://openrouter.ai/api/v1",
        openai_api_key=OPENROUTER_API_KEY,
        # OpenRouter'a Ã¶zel baÅŸlÄ±klarÄ± gÃ¶ndermek iÃ§in (isteÄŸe baÄŸlÄ± ama Ã¶nerilir)
        # headers={
        # "HTTP-Referer": st.secrets.get("YOUR_SITE_URL", "http://localhost:8501"),
        # "X-Title": st.secrets.get("YOUR_APP_NAME", "Streamlit PDF Chatbot")
        # }
        # BazÄ± Langchain versiyonlarÄ± 'chunk_size' bekleyebilir, gerekirse ekleyin.
        # chunk_size=1000 # Ã–rneÄŸin
    )
except Exception as e:
    st.error(f"Embedding modeli yÃ¼klenirken hata oluÅŸtu: {e}")
    st.info(f"KullanÄ±lan Embedding Modeli: {EMBEDDING_MODEL_NAME}. Bu modelin OpenRouter'da 'openai/' prefix'i olmadan da (Ã¶rn: 'text-embedding-ada-002') mevcut olup olmadÄ±ÄŸÄ±nÄ± kontrol edin.")
    st.stop()


# --- YardÄ±mcÄ± Fonksiyonlar ---
def get_pdf_text(pdf_docs):
    """YÃ¼klenen PDF dosyalarÄ±ndan metinleri Ã§Ä±karÄ±r."""
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text() or "" # Sayfada metin yoksa boÅŸ string ekle
    return text

def get_text_chunks(text):
    """Metni daha kÃ¼Ã§Ã¼k, yÃ¶netilebilir parÃ§alara bÃ¶ler."""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # Her bir chunk'Ä±n maksimum karakter sayÄ±sÄ±
        chunk_overlap=200,  # Chunk'lar arasÄ± Ã¶rtÃ¼ÅŸme miktarÄ±
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    return chunks

def get_vector_store(text_chunks):
    """Metin parÃ§alarÄ±ndan embedding oluÅŸturur ve FAISS vektÃ¶r deposu oluÅŸturur."""
    if not text_chunks:
        st.warning("PDF'ten metin Ã§Ä±karÄ±lamadÄ± veya metin boÅŸ.")
        return None
    try:
        vector_store = FAISS.from_texts(texts=text_chunks, embedding=embeddings)
        return vector_store
    except Exception as e:
        st.error(f"VektÃ¶r deposu oluÅŸturulurken hata: {e}")
        st.error(f"Muhtemel Neden: Embedding modeli '{EMBEDDING_MODEL_NAME}' OpenRouter'da bulunamadÄ± veya API anahtarÄ±nÄ±zla ilgili bir sorun var.")
        st.info("OpenRouter model listesini kontrol edin ve 'EMBEDDING_MODEL_NAME' secret'Ä±nÄ± doÄŸru ayarladÄ±ÄŸÄ±nÄ±zdan emin olun.")
        return None

def get_conversational_chain():
    """Soru-cevap iÃ§in LLM zincirini oluÅŸturur ve yapÄ±landÄ±rÄ±r."""
    prompt_template = """
    Sadece aÅŸaÄŸÄ±da verilen baÄŸlamdaki bilgileri kullanarak soruyu yanÄ±tlayÄ±n.
    EÄŸer cevap baÄŸlamda yoksa, "Bilmiyorum, bu bilgi belgede bulunmuyor." deyin.
    Kesinlikle baÄŸlam dÄ±ÅŸÄ± bilgi kullanmayÄ±n veya cevap uydurmayÄ±n.

    BaÄŸlam:
    {context}

    Soru: {question}

    Cevap:"""
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    
    # Langchain'in load_qa_chain'i doÄŸrudan OpenAI client'Ä± almaz,
    # LLM'i Langchain formatÄ±nda sarmallamamÄ±z gerekebilir veya
    # doÄŸrudan OpenRouter API'sine istek atmak iÃ§in bir custom chain yazabiliriz.
    # Åimdilik, Langchain'in temel LLM'leriyle uyumlu bir yapÄ± kullanalÄ±m
    # ve OpenRouter'Ä± Langchain'in OpenAI LLM sarmalayÄ±cÄ±sÄ±yla kullanmaya Ã§alÄ±ÅŸalÄ±m.
    
    # NOT: load_qa_chain normalde Langchain LLM nesnesi bekler.
    # Biz OpenRouter kullandÄ±ÄŸÄ±mÄ±z iÃ§in, yanÄ±tÄ± kendimiz oluÅŸturacaÄŸÄ±z.
    # Bu fonksiyon ÅŸimdilik sadece prompt'u dÃ¶ndÃ¼recek,
    # asÄ±l LLM Ã§aÄŸrÄ±sÄ±nÄ± ana kodda yapacaÄŸÄ±z.
    # Daha geliÅŸmiÅŸ bir Ã§Ã¶zÃ¼m iÃ§in Langchain CustomLLM veya doÄŸrudan API Ã§aÄŸrÄ±sÄ± gerekir.
    return prompt


# --- Streamlit ArayÃ¼zÃ¼ ---
st.set_page_config(page_title="PDF Destekli Chatbot", page_icon="ğŸ“„")
st.header("ğŸ“„ PDF KaynaklÄ± Chatbot")
st.write("Sadece yÃ¼klediÄŸiniz PDF(ler) iÃ§eriÄŸinden sorular sorun.")

# Session state'de deÄŸiÅŸkenleri baÅŸlatma
if "conversation_chain_prompt" not in st.session_state:
    st.session_state.conversation_chain_prompt = None
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "pdf_processed" not in st.session_state:
    st.session_state.pdf_processed = False

with st.sidebar:
    st.subheader("PDF DosyalarÄ±nÄ±z")
    pdf_docs = st.file_uploader("PDF dosyalarÄ±nÄ±zÄ± buraya yÃ¼kleyin ve 'Ä°ÅŸle' butonuna tÄ±klayÄ±n", accept_multiple_files=True, type="pdf")

    if st.button("PDF'leri Ä°ÅŸle"):
        if pdf_docs:
            with st.spinner("PDF'ler iÅŸleniyor... Bu iÅŸlem biraz zaman alabilir."):
                try:
                    raw_text = get_pdf_text(pdf_docs)
                    if not raw_text.strip():
                        st.error("PDF'lerden metin Ã§Ä±karÄ±lamadÄ±. Dosyalar boÅŸ veya okunaksÄ±z olabilir.")
                        st.session_state.pdf_processed = False
                    else:
                        text_chunks = get_text_chunks(raw_text)
                        if not text_chunks:
                            st.error("Metin parÃ§alara ayrÄ±lamadÄ±.")
                            st.session_state.pdf_processed = False
                        else:
                            st.session_state.vector_store = get_vector_store(text_chunks)
                            if st.session_state.vector_store:
                                st.session_state.conversation_chain_prompt = get_conversational_chain()
                                st.session_state.chat_history = [] # PDF deÄŸiÅŸince sohbeti sÄ±fÄ±rla
                                st.session_state.pdf_processed = True
                                st.success("PDF(ler) baÅŸarÄ±yla iÅŸlendi! ArtÄ±k soru sorabilirsiniz.")
                            else:
                                st.error("VektÃ¶r deposu oluÅŸturulamadÄ±. LÃ¼tfen hata mesajlarÄ±nÄ± kontrol edin.")
                                st.session_state.pdf_processed = False
                except Exception as e:
                    st.error(f"PDF iÅŸlenirken bir hata oluÅŸtu: {e}")
                    st.session_state.pdf_processed = False
        else:
            st.warning("LÃ¼tfen en az bir PDF dosyasÄ± yÃ¼kleyin.")

    if st.session_state.pdf_processed:
        if st.button("Sohbeti Temizle ve PDF'i Unut"):
            st.session_state.vector_store = None
            st.session_state.conversation_chain_prompt = None
            st.session_state.chat_history = []
            st.session_state.pdf_processed = False
            st.rerun() # SayfayÄ± yeniden yÃ¼kleyerek arayÃ¼zÃ¼ temizle

st.sidebar.markdown("---")
st.sidebar.info(f"LLM Modeli: {LLM_MODEL_NAME}")
st.sidebar.info(f"Embedding Modeli: {EMBEDDING_MODEL_NAME}")


# Sohbet geÃ§miÅŸini gÃ¶sterme
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ±dan girdi alma
if prompt := st.chat_input("PDF iÃ§eriÄŸi hakkÄ±nda sorun..."):
    if not st.session_state.pdf_processed or not st.session_state.vector_store:
        st.warning("LÃ¼tfen Ã¶nce bir PDF yÃ¼kleyip iÅŸleyin.")
    else:
        # KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± geÃ§miÅŸe ekle ve gÃ¶ster
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Bot'un yanÄ±tÄ±nÄ± alma
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            try:
                # 1. Benzerlik AramasÄ± (Retrieval)
                # KullanÄ±cÄ±nÄ±n sorusuna en Ã§ok benzeyen metin parÃ§alarÄ±nÄ± vektÃ¶r deposundan bul
                docs = st.session_state.vector_store.similarity_search(query=prompt, k=4) # En iyi 4 chunk
                
                if not docs:
                    full_response = "Belgede sorunuzla ilgili bir bilgi bulamadÄ±m."
                else:
                    # 2. Prompt'u HazÄ±rlama (Augmentation)
                    # Bulunan metin parÃ§alarÄ±nÄ± baÄŸlam olarak kullanarak prompt'u oluÅŸtur
                    context_text = "\n\n".join([doc.page_content for doc in docs])
                    
                    # Sistem mesajÄ±nÄ± ve kullanÄ±cÄ± prompt'unu birleÅŸtir
                    # OpenRouter iÃ§in mesaj listesi formatÄ±
                    final_prompt_messages = [
                        {"role": "system", "content": st.session_state.conversation_chain_prompt.template.format(context="DÄ°KKAT: Bu bir yer tutucudur, asÄ±l context aÅŸaÄŸÄ±dadÄ±r.", question="DÄ°KKAT: Bu bir yer tutucudur, asÄ±l soru aÅŸaÄŸÄ±dadÄ±r.")},
                        {"role": "user", "content": f"BaÄŸlam:\n{context_text}\n\nSoru: {prompt}\n\nCevap:"}
                    ]
                    # st.write("LLM'e gÃ¶nderilen mesajlar:", final_prompt_messages) # Debug iÃ§in

                    # 3. YanÄ±t Ãœretme (Generation)
                    response_stream = llm_client.chat.completions.create(
                        model=LLM_MODEL_NAME,
                        messages=final_prompt_messages,
                        stream=True,
                        # extra_headers={ # Gerekirse OpenRouter'a Ã¶zel baÅŸlÄ±klar
                        # "HTTP-Referer": st.secrets.get("YOUR_SITE_URL", "http://localhost:8501"),
                        # "X-Title": st.secrets.get("YOUR_APP_NAME", "Streamlit PDF Chatbot")
                        # }
                    )
                    
                    for chunk in response_stream:
                        if chunk.choices[0].delta and chunk.choices[0].delta.content:
                            full_response += chunk.choices[0].delta.content
                            message_placeholder.markdown(full_response + "â–Œ")
                
                message_placeholder.markdown(full_response)

            except openai.APIError as e: # openai kÃ¼tÃ¼phanesi artÄ±k bu ÅŸekilde
                st.error(f"OpenRouter API HatasÄ±: {e}")
                st.error(f"Detay: {e.body if hasattr(e, 'body') else 'Detay yok'}")
                full_response = "ÃœzgÃ¼nÃ¼m, API ile iletiÅŸimde bir sorun oluÅŸtu."
                message_placeholder.markdown(full_response)
            except Exception as e:
                st.error(f"Beklenmedik bir hata oluÅŸtu: {e}")
                import traceback
                st.error(traceback.format_exc()) # Tam hata izini gÃ¶ster
                full_response = "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu."
                message_placeholder.markdown(full_response)

        # Bot'un yanÄ±tÄ±nÄ± geÃ§miÅŸe ekle
        st.session_state.chat_history.append({"role": "assistant", "content": full_response})
