import streamlit as st
from openai import OpenAI # Google AI kullandÄ±ÄŸÄ±mÄ±z iÃ§in bu aslÄ±nda gerekmeyebilir, ama genel APIError iÃ§in kalabilir.
                          # Veya google.api_core.exceptions gibi Google'a Ã¶zel hatalarÄ± yakalayabilirsiniz.
import os
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings # Google AI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain.output_parsers import PydanticOutputParser # YapÄ±landÄ±rÄ±lmÄ±ÅŸ Ã§Ä±ktÄ± iÃ§in
from pydantic import BaseModel, Field # Pydantic modelleri iÃ§in
from typing import List, Optional, Union # Tip ipuÃ§larÄ± iÃ§in
import traceback
import uuid
import asyncio # <-- GEREKLÄ° EKLEME

# -----------------------------------------------------------------------------
# SAYFA KONFÄ°GÃœRASYONU - Ä°LK STREAMLIT KOMUTU OLMALI!
# -----------------------------------------------------------------------------
st.set_page_config(page_title="Google AI PDF AsistanÄ±", page_icon="âœ¨ğŸ“š")
#bende burdayÄ±m kardeÅŸ
# -----------------------------------------------------------------------------

# --- Streamlit Secrets ve Google AI KonfigÃ¼rasyonu ---
GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY")
GOOGLE_LLM_MODEL_NAME = st.secrets.get("GOOGLE_LLM_MODEL_NAME", "gemini-1.5-flash-latest")
GOOGLE_EMBEDDING_MODEL_NAME = st.secrets.get("GOOGLE_EMBEDDING_MODEL_NAME", "models/embedding-001")

if not GOOGLE_API_KEY:
    st.error("Google API anahtarÄ± bulunamadÄ±! LÃ¼tfen Streamlit Secrets bÃ¶lÃ¼mÃ¼ne 'GOOGLE_API_KEY' olarak ekleyin.")
    st.stop()

# --- Pydantic Modelleri TanÄ±mlama (YapÄ±landÄ±rÄ±lmÄ±ÅŸ Ã‡Ä±ktÄ± Ä°Ã§in) ---
class GeneratedQuestionPydantic(BaseModel):
    question: str = Field(description="PDF iÃ§eriÄŸi hakkÄ±nda anlamlÄ± ve spesifik bir soru.")

class DocumentAnalysisOutput(BaseModel):
    summary: Optional[str] = Field(default=None, description="PDF iÃ§eriÄŸinin kÄ±sa bir Ã¶zeti (3-4 cÃ¼mle).")
    questions: Optional[List[GeneratedQuestionPydantic]] = Field(default=None, description="PDF iÃ§eriÄŸine dayalÄ± olarak tÃ¼retilmiÅŸ 2 ila 3 adet spesifik soru.")

# --- Langchain Output Parser ---
output_parser_global = PydanticOutputParser(pydantic_object=DocumentAnalysisOutput)

# --- LLM ve Embedding Ä°stemcileri ---
@st.cache_resource # Embedding modelini global olarak cache'le
def load_google_embedding_model(api_key, model_name):
    """
    DÃœZELTÄ°LMÄ°Å FONKSÄ°YON: Bu fonksiyon, istemciyi baÅŸlatmadan Ã¶nce
    mevcut iÅŸ parÃ§acÄ±ÄŸÄ± iÃ§in bir asyncio event loop'u olmasÄ±nÄ± saÄŸlar.
    """
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError as e:
        if "no current event loop" in str(e):
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        else:
            raise

    try:
        print(f"Google AI Embedding istemcisi yÃ¼kleniyor: {model_name}")
        model = GoogleGenerativeAIEmbeddings(model=model_name, google_api_key=api_key)
        print("Google AI Embedding istemcisi baÅŸarÄ±yla yÃ¼klendi.")
        return model
    except Exception as e:
        st.error(f"Google AI Embedding modeli ({model_name}) yÃ¼klenirken hata: {e}")
        st.error(traceback.format_exc())
        return None

@st.cache_resource # LLM client'Ä±nÄ± da cache'leyebiliriz
def load_google_llm_client(api_key, model_name):
    """
    DÃœZELTÄ°LMÄ°Å FONKSÄ°YON: Bu fonksiyon, istemciyi baÅŸlatmadan Ã¶nce
    mevcut iÅŸ parÃ§acÄ±ÄŸÄ± iÃ§in bir asyncio event loop'u olmasÄ±nÄ± saÄŸlar.
    """
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError as e:
        if "no current event loop" in str(e):
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        else:
            raise
            
    try:
        print(f"Google AI LLM istemcisi yÃ¼kleniyor: {model_name}")
        client = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=api_key,
            temperature=0.2,
        )
        print("Google AI LLM istemcisi baÅŸarÄ±yla yÃ¼klendi.")
        return client
    except Exception as e:
        st.error(f"Google AI LLM istemcisi ({model_name}) oluÅŸturulurken hata: {e}")
        st.error(traceback.format_exc())
        return None

embeddings_model_global = load_google_embedding_model(GOOGLE_API_KEY, GOOGLE_EMBEDDING_MODEL_NAME)
llm_client_global = load_google_llm_client(GOOGLE_API_KEY, GOOGLE_LLM_MODEL_NAME)

if embeddings_model_global is None or llm_client_global is None:
    st.error("LLM veya Embedding modeli yÃ¼klenemedi. LÃ¼tfen API anahtarÄ±nÄ±zÄ± ve model adlarÄ±nÄ±zÄ± kontrol edin.")
    st.stop()

# --- YardÄ±mcÄ± Fonksiyonlar ---
def get_pdf_text(pdf_docs):
    text = ""
    if pdf_docs:
        for pdf in pdf_docs:
            try:
                pdf_reader = PdfReader(pdf)
                for page in pdf_reader.pages:
                    page_text = page.extract_text();
                    if page_text: text += page_text
            except Exception as e: st.warning(f"'{pdf.name}' dosyasÄ±ndan metin Ã§Ä±karÄ±lÄ±rken hata: {e}")
    return text

def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)
    return text_splitter.split_text(text)

def create_vector_store_from_chunks(text_chunks, current_embeddings_model):
    if not text_chunks or not current_embeddings_model: return None
    try: return FAISS.from_texts(texts=text_chunks, embedding=current_embeddings_model)
    except Exception as e: st.error(f"VektÃ¶r deposu oluÅŸturulurken hata: {e}"); st.error(traceback.format_exc()); return None

# --- Prompt ÅablonlarÄ± ---
@st.cache_data # Prompt template'leri de cache'leyebiliriz, Ã§Ã¼nkÃ¼ deÄŸiÅŸmiyorlar
def get_rag_prompt_template_cached():
    prompt_template_str = """
    SENÄ°N GÃ–REVÄ°N: Sadece ve sadece aÅŸaÄŸÄ±da "BaÄŸlam:" olarak verilen metindeki bilgileri kullanarak "Soru:" kÄ±smÄ±ndaki soruyu yanÄ±tlamaktÄ±r.
    KESÄ°NLÄ°KLE DIÅARIDAN BÄ°LGÄ° KULLANMA, YORUM YAPMA, EK AÃ‡IKLAMA EKLEME VEYA CEVAP UYDURMA.
    CevabÄ±n SADECE ve SADECE "BaÄŸlam:" iÃ§indeki bilgilere dayanmalÄ±dÄ±r.

    EÄŸer "Soru:" kÄ±smÄ±ndaki soruya cevap "BaÄŸlam:" iÃ§inde bulunmuyorsa, ÅŸu cevabÄ± ver:
    "Bu bilgi saÄŸlanan belgede bulunmuyor."
    BU CEVABIN DIÅINDA HÄ°Ã‡BÄ°R ÅEY EKLEME. Ã–rneÄŸin, "Bu bilgi belgede yok ama genel olarak ÅŸÃ¶yledir..." GÄ°BÄ° BÄ°R AÃ‡IKLAMA YAPMA.

    BaÄŸlam:
    {context}

    Soru: {question}

    Cevap:"""
    return PromptTemplate(template=prompt_template_str, input_variables=["context", "question"])

@st.cache_data
def get_structured_analysis_prompt_template_cached(_parser: PydanticOutputParser): # Parser argÃ¼man olarak alÄ±nmalÄ±
    prompt_template_str = """
    SENÄ°N GÃ–REVÄ°N: AÅŸaÄŸÄ±da "BaÄŸlam:" olarak verilen metni analiz etmektir.
    KullanÄ±cÄ± bu belge hakkÄ±nda genel bir fikir edinmek istiyor.

    LÃœTFEN AÅAÄIDAKÄ°LERDEN BÄ°RÄ°NÄ° YAP (SADECE BÄ°RÄ°NÄ° SEÃ‡ VE ONA UYGUN FORMATTA Ã‡IKTI VER):
    1. Metnin ana temalarÄ±nÄ± ve iÃ§eriÄŸini Ã¶zetleyen kÄ±sa (3-4 CÃœMLE) bir aÃ§Ä±klama Ã¼ret. (Bu durumda 'summary' alanÄ±nÄ± doldur.)
    2. VEYA, bu metne dayanarak kullanÄ±cÄ±nÄ±n sorabileceÄŸi 2 Ä°LA 3 ADET anlamlÄ± ve spesifik soru tÃ¼ret. Bu sorular, metnin farklÄ± Ã¶nemli kÄ±sÄ±mlarÄ±nÄ± kapsamalÄ±dÄ±r. (Bu durumda 'questions' alanÄ±nÄ± doldur.)

    Kesinlikle dÄ±ÅŸarÄ±dan bilgi kullanma. Sadece saÄŸlanan baÄŸlamÄ± kullan.

    {format_instructions}

    BaÄŸlam:
    {context}

    Ä°stenen Ã‡Ä±ktÄ± (YukarÄ±daki format talimatlarÄ±na uygun JSON):"""
    return PromptTemplate(
        template=prompt_template_str,
        input_variables=["context"],
        partial_variables={"format_instructions": _parser.get_format_instructions()}
    )

# --- Session State BaÅŸlatma ---
if "sessions" not in st.session_state: st.session_state.sessions = {}
if "current_session_id" not in st.session_state: st.session_state.current_session_id = None
# Prompt template'leri session state'e yÃ¼klemeye gerek yok, cache'li fonksiyonlardan Ã§aÄŸÄ±rabiliriz.

# --- Oturum YÃ¶netimi FonksiyonlarÄ± ---
def create_new_session():
    session_id = str(uuid.uuid4()); session_name = f"Sohbet {len(st.session_state.sessions) + 1}"
    st.session_state.sessions[session_id] = {
        "id": session_id, "name": session_name, "pdf_names": [], "all_text_chunks": [],
        "vector_store": None, "chat_history": [], "pdf_processed": False
    }
    st.session_state.current_session_id = session_id; return session_id

def get_active_session_data():
    if st.session_state.current_session_id and st.session_state.current_session_id in st.session_state.sessions:
        return st.session_state.sessions[st.session_state.current_session_id]
    return None

def delete_session(session_id_to_delete):
    if session_id_to_delete in st.session_state.sessions:
        del st.session_state.sessions[session_id_to_delete]
        if st.session_state.current_session_id == session_id_to_delete:
            st.session_state.current_session_id = None
            if st.session_state.sessions: st.session_state.current_session_id = list(st.session_state.sessions.keys())[0]

# --- Streamlit ArayÃ¼zÃ¼ ---
st.title("âœ¨ Google AI Destekli PDF AsistanÄ± ğŸ“š")

with st.sidebar:
    st.header("Sohbet OturumlarÄ±")
    if st.button("â• Yeni Sohbet BaÅŸlat", key="new_chat_button", use_container_width=True):
        create_new_session()
        st.rerun()

    session_options = {sid: sdata["name"] for sid, sdata in st.session_state.sessions.items()}
    if not session_options and st.session_state.current_session_id is None:
        create_new_session() # Ä°lk oturumu oluÅŸtur
        session_options = {sid: sdata["name"] for sid, sdata in st.session_state.sessions.items()} # SeÃ§enekleri gÃ¼ncelle

    if session_options: # Sadece oturum varsa gÃ¶ster
        selected_session_id = st.selectbox(
            "Aktif Sohbeti SeÃ§in:", options=list(session_options.keys()),
            format_func=lambda sid: session_options.get(sid, "Bilinmeyen Oturum"), # .get() ile daha gÃ¼venli
            index=list(session_options.keys()).index(st.session_state.current_session_id) if st.session_state.current_session_id in session_options else 0,
            key="session_selector"
        )
        if selected_session_id != st.session_state.current_session_id:
            st.session_state.current_session_id = selected_session_id
            st.rerun()

        active_session = get_active_session_data()
        if active_session:
            st.markdown("---"); st.subheader(f"Aktif: {active_session['name']}")
            uploader_key = f"pdf_uploader_{active_session['id']}" # Her oturum iÃ§in farklÄ± key
            uploaded_pdf_docs = st.file_uploader(
                "Bu sohbet iÃ§in PDF dosyalarÄ±nÄ± yÃ¼kleyin:",
                accept_multiple_files=True, type="pdf", key=uploader_key
            )
            if st.button("SeÃ§ili PDF'leri Ä°ÅŸle", key=f"process_btn_{active_session['id']}", use_container_width=True):
                if uploaded_pdf_docs:
                    with st.spinner(f"'{active_session['name']}' iÃ§in PDF'ler iÅŸleniyor..."):
                        active_session["pdf_names"] = [f.name for f in uploaded_pdf_docs]
                        raw_text = get_pdf_text(uploaded_pdf_docs)
                        if not raw_text.strip():
                            st.error("PDF'lerden metin Ã§Ä±karÄ±lamadÄ±."); active_session["all_text_chunks"] = []
                            active_session["vector_store"] = None; active_session["pdf_processed"] = False
                        else:
                            text_chunks = get_text_chunks(raw_text)
                            active_session["all_text_chunks"] = text_chunks # Genel sorular iÃ§in sakla
                            if not text_chunks:
                                st.error("Metin parÃ§alara ayrÄ±lamadÄ±.");
                                active_session["vector_store"] = None; active_session["pdf_processed"] = False
                            else:
                                vector_store = create_vector_store_from_chunks(text_chunks, embeddings_model_global)
                                if vector_store:
                                    active_session["vector_store"] = vector_store
                                    active_session["chat_history"] = [] # Yeni PDF'lerde sohbeti sÄ±fÄ±rla
                                    active_session["pdf_processed"] = True
                                    st.success(f"PDF(ler) '{active_session['name']}' iÃ§in baÅŸarÄ±yla iÅŸlendi.")
                                    st.rerun() # ArayÃ¼zÃ¼ ve sohbet alanÄ±nÄ± gÃ¼ncelle
                                else:
                                    st.error("VektÃ¶r deposu oluÅŸturulamadÄ±."); active_session["pdf_processed"] = False
                else:
                    st.warning("LÃ¼tfen iÅŸlemek iÃ§in en az bir PDF dosyasÄ± yÃ¼kleyin.")
            
            if active_session.get("pdf_processed") and active_session.get("pdf_names"):
                 st.markdown("**Ä°ÅŸlenmiÅŸ PDF(ler):**")
                 for name in active_session["pdf_names"]: st.caption(f"- {name}")

            st.markdown("---")
            if st.button(f"ğŸ—‘ï¸ '{active_session['name']}' Oturumunu Sil", type="secondary", key=f"delete_btn_{active_session['id']}", use_container_width=True):
                session_name_deleted = active_session['name'] # Silmeden Ã¶nce ismi al
                delete_session(active_session['id'])
                st.success(f"'{session_name_deleted}' oturumu silindi."); st.rerun()
    else:
        st.sidebar.info("HenÃ¼z bir sohbet oturumu yok. LÃ¼tfen yeni bir tane baÅŸlatÄ±n.")

# Ana Sohbet AlanÄ±
active_session_data = get_active_session_data()

if active_session_data:
    st.header(f"Sohbet: {active_session_data['name']}")
    # Sohbet geÃ§miÅŸini gÃ¶ster
    for message in active_session_data["chat_history"]:
        with st.chat_message(message["role"]): st.markdown(message["content"])

    # KullanÄ±cÄ±dan girdi al
    if user_query := st.chat_input(f"'{active_session_data['name']}' oturumundaki PDF(ler) hakkÄ±nda sorun..."):
        # Ã–nce PDF'lerin iÅŸlenip iÅŸlenmediÄŸini kontrol et
        pdf_is_ready = active_session_data.get("pdf_processed", False) and active_session_data.get("vector_store") is not None
        general_query_ready = active_session_data.get("pdf_processed", False) and active_session_data.get("all_text_chunks")
        
        general_query_keywords = ["ne anlatÄ±yor", "konusu ne", "Ã¶zetle", "ne bulunur", "neler var", "bahset", "iÃ§eriÄŸi", "genel bakÄ±ÅŸ"]
        is_general_query = any(keyword in user_query.lower() for keyword in general_query_keywords)

        if not (pdf_is_ready or (is_general_query and general_query_ready)):
            st.warning("LÃ¼tfen Ã¶nce bu sohbet iÃ§in PDF yÃ¼kleyip iÅŸleyin.")
        else:
            active_session_data["chat_history"].append({"role": "user", "content": user_query})
            with st.chat_message("user"): st.markdown(user_query)

            with st.chat_message("assistant"):
                message_placeholder = st.empty(); full_response_text = ""
                try:
                    if is_general_query and general_query_ready:
                        st.write("DEBUG: Genel soru algÄ±landÄ±, yapÄ±landÄ±rÄ±lmÄ±ÅŸ analiz yapÄ±lÄ±yor...")
                        # Ä°lk N chunk'Ä± veya belirli bir token limitine kadar olanÄ± al
                        # Bu, modelin context window'una gÃ¶re ayarlanmalÄ±.
                        # Gemini 1.5 Flash bÃ¼yÃ¼k bir context window'a sahip.
                        context_chunks = active_session_data["all_text_chunks"][:20] # Ã–rnek: Ä°lk 20 chunk
                        context_text_for_analysis = "\n\n".join(context_chunks)
                        
                        analysis_prompt_template = get_structured_analysis_prompt_template_cached(output_parser_global)
                        formatted_analysis_prompt = analysis_prompt_template.format(context=context_text_for_analysis)
                        
                        raw_llm_output = ""
                        for chunk in llm_client_global.stream(formatted_analysis_prompt):
                            if hasattr(chunk, 'content'):
                                raw_llm_output += chunk.content
                                message_placeholder.markdown(raw_llm_output + "â–Œ")
                        message_placeholder.markdown(raw_llm_output) # Son ham Ã§Ä±ktÄ±

                        try:
                            # JSON ayÄ±klama (LLM bazen ```json ... ``` bloÄŸu ekler)
                            json_part = raw_llm_output
                            if "```json" in raw_llm_output:
                                json_part = raw_llm_output.split("```json", 1)[1].split("```", 1).strip()
                            elif "```" in raw_llm_output and raw_llm_output.strip().startswith("```") and raw_llm_output.strip().endswith("```"):
                                json_part = raw_llm_output.strip()[3:-3].strip()
                            
                            parsed_output: DocumentAnalysisOutput = output_parser_global.parse(json_part)
                            
                            if parsed_output.summary:
                                full_response_text = f"**Belge Ã–zeti:**\n{parsed_output.summary}"
                            elif parsed_output.questions:
                                questions_md = "**Bu belge hakkÄ±nda sorabileceÄŸiniz bazÄ± sorular:**\n"
                                for i, q_obj in enumerate(parsed_output.questions):
                                    questions_md += f"{i+1}. {q_obj.question}\n"
                                full_response_text = questions_md
                            else: # Ä°kisinden biri olmalÄ±, prompt'a gÃ¶re. Ama fallback.
                                full_response_text = "Belge hakkÄ±nda genel bir Ã¶zet veya soru Ã¼retemedim, ancak LLM'den gelen yanÄ±t yukarÄ±dadÄ±r."
                        except Exception as parse_error:
                            st.error(f"LLM Ã§Ä±ktÄ±sÄ± (analiz iÃ§in) parse edilirken hata: {parse_error}")
                            st.caption("LLM'den gelen ham Ã§Ä±ktÄ± (parse edilemedi):")
                            st.code(raw_llm_output, language='text')
                            full_response_text = "ÃœzgÃ¼nÃ¼m, belge hakkÄ±nda genel bir bilgi veya soru Ã¼retemedim. Ham yanÄ±t yukarÄ±da."
                        message_placeholder.markdown(full_response_text)

                    elif pdf_is_ready: # Spesifik soru (RAG)
                        docs = active_session_data["vector_store"].similarity_search(query=user_query, k=4)
                        if not docs:
                            full_response_text = "Bu bilgi saÄŸlanan belgede bulunmuyor."
                        else:
                            context_text_for_rag = "\n\n".join([doc.page_content for doc in docs])
                            rag_prompt_template = get_rag_prompt_template_cached()
                            formatted_rag_prompt = rag_prompt_template.format(context=context_text_for_rag, question=user_query)
                            
                            for chunk in llm_client_global.stream(formatted_rag_prompt):
                                if hasattr(chunk, 'content'):
                                    full_response_text += chunk.content
                                    message_placeholder.markdown(full_response_text + "â–Œ")
                        message_placeholder.markdown(full_response_text)
                    else: # Bu duruma dÃ¼ÅŸmemeli ama fallback
                        full_response_text = "LÃ¼tfen Ã¶nce PDF yÃ¼kleyip iÅŸleyin."
                        message_placeholder.markdown(full_response_text)

                except Exception as e:
                    st.error(f"YanÄ±t alÄ±nÄ±rken beklenmedik bir hata oluÅŸtu: {e}")
                    st.error(traceback.format_exc()); full_response_text = "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu."
                    message_placeholder.markdown(full_response_text)
            active_session_data["chat_history"].append({"role": "assistant", "content": full_response_text})
else:
    st.info("BaÅŸlamak iÃ§in kenar Ã§ubuÄŸundan bir sohbet seÃ§in veya 'Yeni Sohbet BaÅŸlat'a tÄ±klayÄ±n.")

st.sidebar.markdown("---")
st.sidebar.caption(f"LLM: {GOOGLE_LLM_MODEL_NAME} (Google AI)")
st.sidebar.caption(f"Embedding: {GOOGLE_EMBEDDING_MODEL_NAME} (Google AI)")
