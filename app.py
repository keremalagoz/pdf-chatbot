import streamlit as st

# -----------------------------------------------------------------------------
# SAYFA KONFÄ°GÃœRASYONU - Ä°LK STREAMLIT KOMUTU OLMALI!
# -----------------------------------------------------------------------------
st.set_page_config(page_title="PDF Destekli Chatbot", page_icon="ğŸ“„")
# -----------------------------------------------------------------------------

from openai import OpenAI
import os
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
import traceback

os.environ["TOKENIZERS_PARALLELISM"] = "false"

# --- Streamlit Secrets ve OpenRouter KonfigÃ¼rasyonu ---
OPENROUTER_API_KEY = st.secrets.get("OPENROUTER_API_KEY")
LLM_MODEL_NAME = st.secrets.get("LLM_MODEL_NAME", "mistralai/mistral-7b-instruct:free")
LOCAL_EMBEDDING_MODEL_NAME = st.secrets.get("LOCAL_EMBEDDING_MODEL_NAME", "sentence-transformers/all-MiniLM-L6-v2")

# API anahtarÄ± kontrolÃ¼ st.set_page_config'den sonra olabilir, Ã§Ã¼nkÃ¼ st.error da bir Streamlit komutudur.
if not OPENROUTER_API_KEY:
    # st.set_page_config Ã§aÄŸrÄ±ldÄ±ktan sonra hata mesajÄ± gÃ¶sterilebilir.
    st.error("OpenRouter API anahtarÄ± (LLM iÃ§in) bulunamadÄ±! LÃ¼tfen Streamlit Secrets bÃ¶lÃ¼mÃ¼ne 'OPENROUTER_API_KEY' olarak ekleyin.")
    st.stop() # st.stop() da bir Streamlit komutudur.

llm_client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)

@st.cache_resource
def load_embeddings_model(model_name):
    # Bu fonksiyon iÃ§indeki st.write Ã§aÄŸrÄ±larÄ± st.set_page_config'den sonra sorun olmaz.
    # Ancak, bu fonksiyonun KENDÄ°SÄ° st.set_page_config'den Ã–NCE Ã§aÄŸrÄ±lÄ±rsa ve iÃ§inde st.write varsa sorun olur.
    # Bu yÃ¼zden st.set_page_config'i en baÅŸa aldÄ±k.
    # Ä°sterseniz bu st.write'larÄ± kaldÄ±rabilir veya loglama kÃ¼tÃ¼phanesi kullanabilirsiniz.
    # st.write(f"Yerel embedding modeli yÃ¼kleniyor: {model_name}") # KullanÄ±cÄ± arayÃ¼zÃ¼nÃ¼ kirletmemek iÃ§in loga yazmak daha iyi
    print(f"Yerel embedding modeli yÃ¼kleniyor: {model_name}") # Konsola yazdÄ±rma
    try:
        embeddings_instance = HuggingFaceEmbeddings(
            model_name=model_name,
        )
        # st.write("Yerel embedding modeli baÅŸarÄ±yla yÃ¼klendi.")
        print("Yerel embedding modeli baÅŸarÄ±yla yÃ¼klendi.") # Konsola yazdÄ±rma
        return embeddings_instance
    except Exception as e:
        # st.set_page_config Ã§aÄŸrÄ±ldÄ±ktan sonra st.error kullanÄ±labilir.
        st.error(f"Yerel embedding modeli ({model_name}) yÃ¼klenirken hata oluÅŸtu: {e}")
        st.error(traceback.format_exc())
        st.info("Model adÄ±nÄ±n doÄŸru olduÄŸundan ve 'sentence-transformers', 'torch' kÃ¼tÃ¼phanelerinin kurulu olduÄŸundan emin olun.")
        return None

embeddings = load_embeddings_model(LOCAL_EMBEDDING_MODEL_NAME)

if embeddings is None:
    st.stop() # st.stop() bir Streamlit komutudur, st.set_page_config'den sonra olmalÄ±.

# --- Streamlit ArayÃ¼z BaÅŸlÄ±ÄŸÄ± (st.set_page_config'den sonra) ---
st.header("ğŸ“„ PDF KaynaklÄ± Chatbot")
st.write("Sadece yÃ¼klediÄŸiniz PDF(ler) iÃ§eriÄŸinden sorular sorun.")
# --- (Kalan kod aynÄ±) ---

# ... (Bir Ã¶nceki cevaptaki geri kalan kodun tamamÄ± buraya gelecek) ...
# get_pdf_text, get_text_chunks, create_vector_store, get_conversational_chain_prompt,
# session state baÅŸlatmalarÄ±, sidebar, sohbet geÃ§miÅŸi ve kullanÄ±cÄ± girdi mantÄ±ÄŸÄ±...
# Bu kÄ±sÄ±mlarda st.set_page_config'den Ã¶nce bir Streamlit komutu olmamasÄ±na dikkat edin.
# Geri kalan tÃ¼m kod, st.set_page_config Ã§aÄŸrÄ±sÄ±ndan SONRA gelmelidir.

# (Bir Ã¶nceki cevaptaki geri kalan kodun tamamÄ±nÄ± buraya yapÄ±ÅŸtÄ±rÄ±n)
# ...

def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text() or ""
    return text

def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    return chunks

@st.cache_resource
def create_vector_store(_text_chunks, _embeddings_model):
    if not _text_chunks:
        # st.warning bir Streamlit komutudur, st.set_page_config'den sonra olmalÄ±.
        st.warning("PDF'ten metin Ã§Ä±karÄ±lamadÄ± veya metin boÅŸ.")
        return None
    try:
        vector_store_instance = FAISS.from_texts(texts=_text_chunks, embedding=_embeddings_model)
        return vector_store_instance
    except Exception as e:
        st.error(f"VektÃ¶r deposu oluÅŸturulurken hata: {e}")
        st.error(traceback.format_exc())
        return None

def get_conversational_chain_prompt():
    prompt_template_str = """
    Sadece aÅŸaÄŸÄ±da verilen baÄŸlamdaki bilgileri kullanarak soruyu yanÄ±tlayÄ±n.
    EÄŸer cevap baÄŸlamda yoksa, "Bilmiyorum, bu bilgi belgede bulunmuyor." deyin.
    Kesinlikle baÄŸlam dÄ±ÅŸÄ± bilgi kullanmayÄ±n veya cevap uydurmayÄ±n.

    BaÄŸlam:
    {context}

    Soru: {question}

    Cevap:"""
    prompt = PromptTemplate(template=prompt_template_str, input_variables=["context", "question"])
    return prompt

# Session state baÅŸlatmalarÄ± st.set_page_config'den sonra olmalÄ±
if "conversation_chain_prompt" not in st.session_state:
    st.session_state.conversation_chain_prompt = get_conversational_chain_prompt()
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "pdf_processed" not in st.session_state:
    st.session_state.pdf_processed = False

with st.sidebar: # st.sidebar bir Streamlit komutudur
    st.subheader("PDF DosyalarÄ±nÄ±z") # st.subheader bir Streamlit komutudur
    pdf_docs = st.file_uploader("PDF dosyalarÄ±nÄ±zÄ± buraya yÃ¼kleyin ve 'Ä°ÅŸle' butonuna tÄ±klayÄ±n", accept_multiple_files=True, type="pdf")

    if st.button("PDF'leri Ä°ÅŸle", key="process_pdf_button"):
        if pdf_docs:
            with st.spinner("PDF'ler iÅŸleniyor... Bu iÅŸlem biraz zaman alabilir."): # st.spinner bir Streamlit komutudur
                try:
                    raw_text = get_pdf_text(pdf_docs)
                    if not raw_text.strip():
                        st.error("PDF'lerden metin Ã§Ä±karÄ±lamadÄ±. Dosyalar boÅŸ veya okunaksÄ±z olabilir.")
                        st.session_state.pdf_processed = False
                    else:
                        text_chunks = get_text_chunks(raw_text)
                        if not text_chunks:
                            st.error("Metin parÃ§alara ayrÄ±lamadÄ±.")
                            st.session_state.pdf_processed = False
                        else:
                            st.session_state.vector_store = create_vector_store(text_chunks, embeddings)
                            if st.session_state.vector_store:
                                st.session_state.chat_history = []
                                st.session_state.pdf_processed = True
                                st.success("PDF(ler) baÅŸarÄ±yla iÅŸlendi! ArtÄ±k soru sorabilirsiniz.")
                            else:
                                st.error("VektÃ¶r deposu oluÅŸturulamadÄ±. LÃ¼tfen hata mesajlarÄ±nÄ± kontrol edin.")
                                st.session_state.pdf_processed = False
                except Exception as e:
                    st.error(f"PDF iÅŸlenirken bir hata oluÅŸtu: {e}")
                    st.error(traceback.format_exc())
                    st.session_state.pdf_processed = False
        else:
            st.warning("LÃ¼tfen en az bir PDF dosyasÄ± yÃ¼kleyin.")

    if st.session_state.pdf_processed:
        if st.button("Sohbeti Temizle ve PDF'i Unut", key="clear_chat_button"):
            st.session_state.vector_store = None
            st.session_state.chat_history = []
            st.session_state.pdf_processed = False
            st.rerun() # st.rerun bir Streamlit komutudur

st.sidebar.markdown("---")
st.sidebar.info(f"LLM Modeli: {LLM_MODEL_NAME}")
st.sidebar.info(f"Embedding Modeli: {LOCAL_EMBEDDING_MODEL_NAME} (Yerel)")

for message in st.session_state.chat_history:
    with st.chat_message(message["role"]): # st.chat_message bir Streamlit komutudur
        st.markdown(message["content"])

if user_query := st.chat_input("PDF iÃ§eriÄŸi hakkÄ±nda sorun..."): # st.chat_input bir Streamlit komutudur
    if not st.session_state.pdf_processed or not st.session_state.vector_store:
        st.warning("LÃ¼tfen Ã¶nce bir PDF yÃ¼kleyip iÅŸleyin.")
    else:
        st.session_state.chat_history.append({"role": "user", "content": user_query})
        with st.chat_message("user"):
            st.markdown(user_query)

        with st.chat_message("assistant"):
            message_placeholder = st.empty() # st.empty bir Streamlit komutudur
            full_response_text = ""
            try:
                docs = st.session_state.vector_store.similarity_search(query=user_query, k=4)
                
                if not docs:
                    full_response_text = "Belgede sorunuzla ilgili bir bilgi bulamadÄ±m."
                else:
                    context_text = "\n\n".join([doc.page_content for doc in docs])
                    current_prompt_template = st.session_state.conversation_chain_prompt
                    
                    messages_for_llm = [
                        {"role": "system", "content": current_prompt_template.template.split("Soru:")[0].strip()},
                        {"role": "user", "content": f"BaÄŸlam:\n{context_text}\n\nSoru: {user_query}\n\nCevap:"}
                    ]
                    
                    response_stream = llm_client.chat.completions.create(
                        model=LLM_MODEL_NAME,
                        messages=messages_for_llm,
                        stream=True,
                    )
                    
                    for chunk in response_stream:
                        if chunk.choices[0].delta and chunk.choices[0].delta.content:
                            full_response_text += chunk.choices[0].delta.content
                            message_placeholder.markdown(full_response_text + "â–Œ")
                
                message_placeholder.markdown(full_response_text)

            except OpenAI.APIError as e:
                st.error(f"OpenRouter API HatasÄ±: {e}")
                st.error(f"Detay: {e.body if hasattr(e, 'body') else 'Detay yok'}")
                full_response_text = "ÃœzgÃ¼nÃ¼m, API ile iletiÅŸimde bir sorun oluÅŸtu."
                message_placeholder.markdown(full_response_text)
            except Exception as e:
                st.error(f"Beklenmedik bir hata oluÅŸtu: {e}")
                st.error(traceback.format_exc())
                full_response_text = "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu."
                message_placeholder.markdown(full_response_text)

        st.session_state.chat_history.append({"role": "assistant", "content": full_response_text})
